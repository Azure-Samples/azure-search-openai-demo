# Ocena jakoÅ›ci odpowiedzi RAG

[ğŸ“º OglÄ…daj: (seria RAG Deep Dive) Ocena jakoÅ›ci odpowiedzi RAG](https://www.youtube.com/watch?v=lyCLu53fb3g)

PostÄ™puj zgodnie z tymi krokami, aby oceniÄ‡ jakoÅ›Ä‡ odpowiedzi generowanych przez przepÅ‚yw RAG.

* [WdrÃ³Å¼ model oceny](#wdrÃ³Å¼-model-oceny)
* [Skonfiguruj Å›rodowisko oceny](#skonfiguruj-Å›rodowisko-oceny)
* [Wygeneruj dane referencyjne](#wygeneruj-dane-referencyjne)
* [Uruchom ocenÄ™ zbiorczÄ…](#uruchom-ocenÄ™-zbiorczÄ…)
* [Przejrzyj wyniki oceny](#przejrzyj-wyniki-oceny)
* [Uruchom ocenÄ™ zbiorczÄ… na PR](#uruchom-ocenÄ™-zbiorczÄ…-na-pr)

## WdrÃ³Å¼ model oceny

1. Uruchom to polecenie, aby powiedzieÄ‡ `azd`, aby wdroÅ¼yÄ‡ model poziomu GPT-4 do oceny:

    ```shell
    azd env set USE_EVAL true
    ```

2. Ustaw pojemnoÅ›Ä‡ na najwyÅ¼szÄ… moÅ¼liwÄ… wartoÅ›Ä‡, aby upewniÄ‡ siÄ™, Å¼e ocena przebiega stosunkowo szybko. Nawet przy wysokiej pojemnoÅ›ci moÅ¼e zajÄ…Ä‡ duÅ¼o czasu wygenerowanie danych referencyjnych i uruchomienie ocen zbiorczych.

    ```shell
    azd env set AZURE_OPENAI_EVAL_DEPLOYMENT_CAPACITY 100
    ```

    DomyÅ›lnie zaaprowizuje to model `gpt-4o`, wersja `2024-08-06`. Aby zmieniÄ‡ te ustawienia, ustaw zmienne Å›rodowiskowe azd `AZURE_OPENAI_EVAL_MODEL` i `AZURE_OPENAI_EVAL_MODEL_VERSION` na Å¼Ä…dane wartoÅ›ci.

3. NastÄ™pnie uruchom nastÄ™pujÄ…ce polecenie, aby aprowizowaÄ‡ model:

    ```shell
    azd provision
    ```

## Skonfiguruj Å›rodowisko oceny

UtwÃ³rz nowe Å›rodowisko wirtualne Python i aktywuj je. Jest to obecnie wymagane ze wzglÄ™du na niekompatybilnoÅ›ci miÄ™dzy zaleÅ¼noÅ›ciami skryptu oceny a gÅ‚Ã³wnym projektem.

```bash
python -m venv .evalenv
```

```bash
source .evalenv/bin/activate
```

Zainstaluj wszystkie zaleÅ¼noÅ›ci dla skryptu oceny, uruchamiajÄ…c nastÄ™pujÄ…ce polecenie:

```bash
pip install -r evals/requirements.txt
```

## Wygeneruj dane referencyjne

Wygeneruj dane referencyjne, uruchamiajÄ…c nastÄ™pujÄ…ce polecenie:

```bash
python evals/generate_ground_truth.py --numquestions=200 --numsearchdocs=1000
```

Opcje to:

* `numquestions`: Liczba pytaÅ„ do wygenerowania. Sugerujemy co najmniej 200.
* `numsearchdocs`: Liczba dokumentÃ³w (fragmentÃ³w) do pobrania z indeksu wyszukiwania. MoÅ¼esz pominÄ…Ä‡ opcjÄ™, aby pobraÄ‡ wszystkie dokumenty, ale to znacznie zwiÄ™kszy czas potrzebny na wygenerowanie danych referencyjnych. MoÅ¼esz chcieÄ‡ przynajmniej zaczÄ…Ä‡ od podzbioru.
* `kgfile`: IstniejÄ…cy plik JSON bazy wiedzy RAGAS, ktÃ³ry zazwyczaj nazywa siÄ™ `ground_truth_kg.json`. MoÅ¼esz chcieÄ‡ to okreÅ›liÄ‡, jeÅ›li juÅ¼ utworzyÅ‚eÅ› bazÄ™ wiedzy i chcesz tylko dostosowaÄ‡ kroki generowania pytaÅ„.
* `groundtruthfile`: Plik do zapisania wygenerowanych odpowiedzi referencyjnych. DomyÅ›lnie jest to `evals/ground_truth.jsonl`.

ğŸ•°ï¸ To moÅ¼e zajÄ…Ä‡ duÅ¼o czasu, moÅ¼liwie kilka godzin, w zaleÅ¼noÅ›ci od rozmiaru indeksu wyszukiwania.

Przejrzyj wygenerowane dane w `evals/ground_truth.jsonl` po uruchomieniu tego skryptu, usuwajÄ…c wszelkie pary pytanie/odpowiedÅº, ktÃ³re nie wydajÄ… siÄ™ realistycznym wejÅ›ciem uÅ¼ytkownika.

## Uruchom ocenÄ™ zbiorczÄ…

Przejrzyj konfiguracjÄ™ w `evals/evaluate_config.json`, aby upewniÄ‡ siÄ™, Å¼e wszystko jest poprawnie skonfigurowane. MoÅ¼esz chcieÄ‡ dostosowaÄ‡ uÅ¼ywane metryki. Zobacz [README ai-rag-chat-evaluator](https://github.com/Azure-Samples/ai-rag-chat-evaluator) po wiÄ™cej informacji o dostÄ™pnych metrykach.

DomyÅ›lnie skrypt oceny bÄ™dzie oceniaÅ‚ kaÅ¼de pytanie w danych referencyjnych.
Uruchom skrypt oceny, uruchamiajÄ…c nastÄ™pujÄ…ce polecenie:

```bash
python evals/evaluate.py
```

Opcje to:

* `numquestions`: Liczba pytaÅ„ do oceny. DomyÅ›lnie sÄ… to wszystkie pytania w danych referencyjnych.
* `resultsdir`: Katalog do zapisania wynikÃ³w oceny. DomyÅ›lnie jest to folder z znacznikiem czasu w `evals/results`. Ta opcja moÅ¼e byÄ‡ rÃ³wnieÅ¼ okreÅ›lona w `evaluate_config.json`.
* `targeturl`: URL dziaÅ‚ajÄ…cej aplikacji do oceny. DomyÅ›lnie jest to `http://localhost:50505`. Ta opcja moÅ¼e byÄ‡ rÃ³wnieÅ¼ okreÅ›lona w `evaluate_config.json`.

ğŸ•°ï¸ To moÅ¼e zajÄ…Ä‡ duÅ¼o czasu, moÅ¼liwie kilka godzin, w zaleÅ¼noÅ›ci od liczby pytaÅ„ referencyjnych, pojemnoÅ›ci TPM modelu oceny i liczby Å¼Ä…danych metryk opartych na LLM.

## Przejrzyj wyniki oceny

Skrypt oceny wyprowadzi podsumowanie wynikÃ³w oceny wewnÄ…trz katalogu `evals/results`.

MoÅ¼esz zobaczyÄ‡ podsumowanie wynikÃ³w ze wszystkich przebiegÃ³w oceny, uruchamiajÄ…c nastÄ™pujÄ…ce polecenie:

```bash
python -m evaltools summary evals/results
```

PorÃ³wnaj odpowiedzi z danymi referencyjnymi, uruchamiajÄ…c nastÄ™pujÄ…ce polecenie:

```bash
python -m evaltools diff evals/results/baseline/
```

PorÃ³wnaj odpowiedzi w dwÃ³ch przebiegach, uruchamiajÄ…c nastÄ™pujÄ…ce polecenie:

```bash
python -m evaltools diff evals/results/baseline/ evals/results/SECONDRUNHERE
```

## Uruchom ocenÄ™ zbiorczÄ… na PR

To repozytorium zawiera przepÅ‚yw pracy GitHub Action `evaluate.yaml`, ktÃ³ry moÅ¼e byÄ‡ uÅ¼ywany do uruchomienia oceny zmian w PR.

Aby przepÅ‚yw pracy dziaÅ‚aÅ‚ pomyÅ›lnie, musisz najpierw skonfigurowaÄ‡ [ciÄ…gÅ‚Ä… integracjÄ™](./azd.md#github-actions) dla repozytorium.

Aby uruchomiÄ‡ ocenÄ™ zmian w PR, czÅ‚onek repozytorium moÅ¼e opublikowaÄ‡ komentarz `/evaluate` do PR. To wyzwoli przepÅ‚yw pracy oceny, aby uruchomiÄ‡ ocenÄ™ zmian PR i opublikuje wyniki do PR.

## OceÅ„ odpowiedzi RAG multimodalnego

Repozytorium zawiera rÃ³wnieÅ¼ plik `evaluate_config_multimodal.json` specjalnie do oceny odpowiedzi RAG multimodalnego. Ta konfiguracja uÅ¼ywa innego pliku referencyjnego, `ground_truth_multimodal.jsonl`, ktÃ³ry zawiera pytania oparte na przykÅ‚adowych danych, ktÃ³re wymagajÄ… zarÃ³wno ÅºrÃ³deÅ‚ tekstowych, jak i obrazowych do odpowiedzi.

NaleÅ¼y pamiÄ™taÄ‡, Å¼e ewaluator "groundedness" nie jest niezawodny dla RAG multimodalnego, poniewaÅ¼ obecnie nie uwzglÄ™dnia ÅºrÃ³deÅ‚ obrazÃ³w. Nadal uwzglÄ™dniamy go w metrykach, ale bardziej niezawodne metryki to "relevance" i "citations matched".
