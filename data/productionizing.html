<h1 id="rag-chat-productionizing-the-app">RAG chat: Productionizing the
app</h1>
<p>This sample is designed to be a starting point for your own
production application, but you should do a thorough review of the
security and performance before deploying to production. Here are some
things to consider:</p>
<ul>
<li><a href="#azure-resource-configuration">Azure resource
configuration</a></li>
<li><a href="#additional-security-measures">Additional security
measures</a></li>
<li><a href="#load-testing">Load testing</a></li>
<li><a href="#evaluation">Evaluation</a></li>
</ul>
<h2 id="azure-resource-configuration">Azure resource configuration</h2>
<h3 id="openai-capacity">OpenAI Capacity</h3>
<p>The default TPM (tokens per minute) is set to 30K. That is equivalent
to approximately 30 conversations per minute (assuming 1K per user
message/response). You can increase the capacity by changing the
<code>chatGptDeploymentCapacity</code> and
<code>embeddingDeploymentCapacity</code> parameters in
<code>infra/main.bicep</code> to your account’s maximum capacity. You
can also view the Quotas tab in <a href="https://oai.azure.com/">Azure
OpenAI studio</a> to understand how much capacity you have.</p>
<p>If the maximum TPM isn’t enough for your expected load, you have a
few options:</p>
<ul>
<li><p>Use a backoff mechanism to retry the request. This is helpful if
you’re running into a short-term quota due to bursts of activity but
aren’t over long-term quota. The <a
href="https://tenacity.readthedocs.io/en/latest/">tenacity</a> library
is a good option for this, and this <a
href="https://github.com/Azure-Samples/azure-search-openai-demo/pull/500">pull
request</a> shows how to apply it to this app.</p></li>
<li><p>If you are consistently going over the TPM, then consider
implementing a load balancer between OpenAI instances. Most developers
implement that using Azure API Management or container-based load
balancers. A native Python approach that integrates with the OpenAI
Python API Library is also possible. For integration instructions with
this sample, please check:</p>
<ul>
<li><a
href="https://learn.microsoft.com/azure/developer/python/get-started-app-chat-scaling-with-azure-api-management">Scale
Azure OpenAI for Python with Azure API Management</a></li>
<li><a
href="https://learn.microsoft.com/azure/developer/python/get-started-app-chat-scaling-with-azure-container-apps">Scale
Azure OpenAI for Python chat using RAG with Azure Container
Apps</a></li>
<li><a
href="https://github.com/Azure-Samples/azure-search-openai-demo/pull/1626">Pull
request: Scale Azure OpenAI for Python with the Python
openai-priority-loadbalancer</a></li>
</ul></li>
</ul>
<h3 id="azure-storage">Azure Storage</h3>
<p>The default storage account uses the <code>Standard_LRS</code> SKU.
To improve your resiliency, we recommend using <code>Standard_ZRS</code>
for production deployments, which you can specify using the
<code>sku</code> property under the <code>storage</code> module in
<code>infra/main.bicep</code>.</p>
<h3 id="azure-ai-search">Azure AI Search</h3>
<p>The default search service uses the “Basic” SKU with the free
semantic ranker option, which gives you 1000 free queries a month. After
1000 queries, you will get an error message about exceeding the semantic
ranker free capacity.</p>
<ul>
<li><p>Assuming your app will experience more than 1000 questions per
month, you should upgrade the semantic ranker SKU from “free” to
“standard” SKU:</p>
<pre class="shell"><code>azd env set AZURE_SEARCH_SEMANTIC_RANKER standard</code></pre>
<p>Or disable semantic search entirely:</p>
<pre class="shell"><code>azd env set AZURE_SEARCH_SEMANTIC_RANKER disabled</code></pre></li>
<li><p>The search service can handle fairly large indexes, but it does
have per-SKU limits on storage sizes, maximum vector dimensions, etc.
You may want to upgrade the SKU to either a Standard or Storage
Optimized SKU, depending on your expected load. However, you <a
href="https://learn.microsoft.com/azure/search/search-sku-tier#tier-upgrade-or-downgrade">cannot
change the SKU</a> of an existing search service, so you will need to
re-index the data or manually copy it over. You can change the SKU by
setting the <code>AZURE_SEARCH_SERVICE_SKU</code> azd environment
variable to <a
href="https://learn.microsoft.com/azure/templates/microsoft.search/searchservices?pivots=deployment-language-bicep#sku">an
allowed SKU</a>.</p>
<pre class="shell"><code>azd env set AZURE_SEARCH_SERVICE_SKU standard</code></pre>
<p>See the <a
href="https://learn.microsoft.com/azure/search/search-limits-quotas-capacity">Azure
AI Search service limits documentation</a> for more details.</p></li>
<li><p>If you see errors about search service capacity being exceeded,
you may find it helpful to increase the number of replicas by changing
<code>replicaCount</code> in
<code>infra/core/search/search-services.bicep</code> or manually scaling
it from the Azure Portal.</p></li>
</ul>
<h3 id="azure-app-service">Azure App Service</h3>
<p>The default app service plan uses the <code>Basic</code> SKU with 1
CPU core and 1.75 GB RAM. We recommend using a Premium level SKU,
starting with 1 CPU core. You can use auto-scaling rules or scheduled
scaling rules, and scale up the maximum/minimum based on load.</p>
<h2 id="additional-security-measures">Additional security measures</h2>
<ul>
<li><strong>Authentication</strong>: By default, the deployed app is
publicly accessible. We recommend restricting access to authenticated
users. See <a
href="./deploy_features.md#enabling-authentication">Enabling
authentication</a> to learn how to enable authentication.</li>
<li><strong>Networking</strong>: We recommend <a
href="./deploy_private.md">deploying inside a Virtual Network</a>. If
the app is only for internal enterprise use, use a private DNS zone.
Also consider using Azure API Management (APIM) for firewalls and other
forms of protection. For more details, read <a
href="https://techcommunity.microsoft.com/blog/azurearchitectureblog/azure-openai-landing-zone-reference-architecture/3882102">Azure
OpenAI Landing Zone reference architecture</a>.</li>
</ul>
<h2 id="load-testing">Load testing</h2>
<p>We recommend running a loadtest for your expected number of users.
You can use the <a href="https://docs.locust.io/">locust tool</a> with
the <code>locustfile.py</code> in this sample or set up a loadtest with
Azure Load Testing.</p>
<p>To use locust, first install the dev requirements that includes
locust:</p>
<pre class="shell"><code>python -m pip install -r requirements-dev.txt</code></pre>
<p>Or manually install locust:</p>
<pre class="shell"><code>python -m pip install locust</code></pre>
<p>Then run the locust command, specifying the name of the User class to
use from <code>locustfile.py</code>. We’ve provided a
<code>ChatUser</code> class that simulates a user asking questions and
receiving answers, as well as a <code>ChatVisionUser</code> to simulate
a user asking questions with the <a href="/docs/gpt4v.md">GPT-4 vision
mode enabled</a>.</p>
<pre class="shell"><code>locust ChatUser</code></pre>
<p>Open the locust UI at <a
href="http://localhost:8089/">http://localhost:8089/</a>, the URI
displayed in the terminal.</p>
<p>Start a new test with the URI of your website,
e.g. <code>https://my-chat-app.azurewebsites.net</code>. Do <em>not</em>
end the URI with a slash. You can start by pointing at your localhost if
you’re concerned more about load on OpenAI/AI Search than the host
platform.</p>
<p>For the number of users and spawn rate, we recommend starting with 20
users and 1 users/second. From there, you can keep increasing the number
of users to simulate your expected load.</p>
<p>Here’s an example loadtest for 50 users and a spawn rate of 1 per
second:</p>
<figure>
<img src="images/screenshot_locust.png"
alt="Screenshot of Locust charts showing 5 requests per second" />
<figcaption aria-hidden="true">Screenshot of Locust charts showing 5
requests per second</figcaption>
</figure>
<p>After each test, check the local or App Service logs to see if there
are any errors.</p>
<h2 id="evaluation">Evaluation</h2>
<p>Before you make your chat app available to users, you’ll want to
rigorously evaluate the answer quality. You can use tools in <a
href="https://github.com/Azure-Samples/ai-rag-chat-evaluator">the AI RAG
Chat evaluator</a> repository to run evaluations, review results, and
compare answers across runs.</p>
