<h1 id="rag-chat-using-gpt-vision-model-with-rag-approach">RAG chat:
Using GPT vision model with RAG approach</h1>
<p><a href="https://www.youtube.com/watch?v=5FfIy7G2WW0">üì∫ Watch: (RAG
Deep Dive series) Multimedia data ingestion</a></p>
<p>This repository includes an optional feature that uses the GPT vision
model to generate responses based on retrieved content. This feature is
useful for answering questions based on the visual content of documents,
such as photos and charts.</p>
<h2 id="how-it-works">How it works</h2>
<p>When this feature is enabled, the following changes are made to the
application:</p>
<ul>
<li><strong>Search index</strong>: We added a new field to the Azure AI
Search index to store the embedding returned by the multimodal Azure AI
Vision API (while keeping the existing field that stores the OpenAI text
embeddings).</li>
<li><strong>Data ingestion</strong>: In addition to our usual PDF
ingestion flow, we also convert each PDF document page to an image,
store that image with the filename rendered on top, and add the
embedding to the index.</li>
<li><strong>Question answering</strong>: We search the index using both
the text and multimodal embeddings. We send both the text and the image
to gpt-4o, and ask it to answer the question based on both kinds of
sources.</li>
<li><strong>Citations</strong>: The frontend displays both image sources
and text sources, to help users understand how the answer was
generated.</li>
</ul>
<p>For more details on how this feature works, read <a
href="https://techcommunity.microsoft.com/blog/azuredevcommunityblog/integrating-vision-into-rag-applications/4239460">this
blog post</a> or watch <a
href="https://www.youtube.com/live/C3Zq3z4UQm4?si=SSPowBBJoTBKZ9WW&amp;t=89">this
video</a>.</p>
<h2 id="using-the-feature">Using the feature</h2>
<h3 id="prerequisites">Prerequisites</h3>
<ul>
<li>Create a <a
href="https://ms.portal.azure.com/#create/Microsoft.CognitiveServicesComputerVision">AI
Vision account in Azure Portal first</a>, so that you can agree to the
Responsible AI terms for that resource. You can delete that account
after agreeing.</li>
<li>The ability to deploy a gpt-4o model in the <a
href="https://learn.microsoft.com/azure/ai-services/openai/concepts/models#standard-deployment-model-availability">supported
regions</a>. If you‚Äôre not sure, try to create a gpt-4o deployment from
your Azure OpenAI deployments page.</li>
<li>Ensure that you can deploy the Azure OpenAI resource group in <a
href="https://learn.microsoft.com/azure/cognitive-services/openai/concepts/models#model-summary-table-and-region-availability">a
region where all required components are available</a>:
<ul>
<li>Azure OpenAI models
<ul>
<li>gpt-35-turbo</li>
<li>text-embedding-ada-002</li>
<li>gpt-4o</li>
</ul></li>
<li><a
href="https://learn.microsoft.com/azure/ai-services/computer-vision/">Azure
AI Vision</a></li>
</ul></li>
</ul>
<h3 id="deployment">Deployment</h3>
<ol type="1">
<li><p><strong>Enable GPT vision approach:</strong></p>
<p>First, make sure you do <em>not</em> have integrated vectorization
enabled, since that is currently incompatible:</p>
<pre class="shell"><code>azd env set USE_FEATURE_INT_VECTORIZATION false</code></pre>
<p>Then set the environment variable for enabling vision support:</p>
<pre class="shell"><code>azd env set USE_GPT4V true</code></pre>
<p>When set, that flag will provision a Azure AI Vision resource and
gpt-4o model, upload image versions of PDFs to Blob storage, upload
embeddings of images in a new <code>imageEmbedding</code> field, and
enable the vision approach in the UI.</p></li>
<li><p><strong>Clean old deployments (optional):</strong> Run
<code>azd down --purge</code> for a fresh setup.</p></li>
<li><p><strong>Start the application:</strong> Execute
<code>azd up</code> to build, provision, deploy, and initiate document
preparation.</p></li>
<li><p><strong>Try out the feature:</strong></p>
<ul>
<li>Access the developer options in the web app and select ‚ÄúUse GPT
vision model‚Äù.</li>
<li>New sample questions will show up in the UI that are based on the
sample financial document.</li>
<li>Try out a question and see the answer generated by the GPT vision
model.</li>
<li>Check the ‚ÄòThought process‚Äô and ‚ÄòSupporting content‚Äô tabs.</li>
</ul></li>
</ol>
