<h1 id="rag-chat-data-ingestion">RAG chat: Data ingestion</h1>
<p>The <a href="/">azure-search-openai-demo</a> project can set up a
full RAG chat app on Azure AI Search and OpenAI so that you can chat on
custom data, like internal enterprise data or domain-specific knowledge
sets. For full instructions on setting up the project, consult the <a
href="/README.md">main README</a>, and then return here for detailed
instructions on the data ingestion component.</p>
<p>The chat app provides two ways to ingest data: manual indexing and
integrated vectorization. This document explains the differences between
the two approaches and provides an overview of the manual indexing
process.</p>
<ul>
<li><a href="#supported-document-formats">Supported document
formats</a></li>
<li><a href="#manual-indexing-process">Manual indexing process</a>
<ul>
<li><a href="#chunking">Chunking</a></li>
<li><a
href="#enhancing-search-functionality-with-data-categorization">Categorizing
data for enhanced search</a></li>
<li><a href="#indexing-additional-documents">Indexing additional
documents</a></li>
<li><a href="#removing-documents">Removing documents</a></li>
</ul></li>
<li><a href="#integrated-vectorization">Integrated Vectorization</a>
<ul>
<li><a href="#indexing-of-additional-documents">Indexing of additional
documents</a></li>
<li><a href="#removal-of-documents">Removal of documents</a></li>
<li><a href="#scheduled-indexing">Scheduled indexing</a></li>
</ul></li>
<li><a href="#debugging-tips">Debugging tips</a></li>
</ul>
<h2 id="supported-document-formats">Supported document formats</h2>
<p>In order to ingest a document format, we need a tool that can turn it
into text. By default, the manual indexing uses Azure Document
Intelligence (DI in the table below), but we also have local parsers for
several formats. The local parsers are not as sophisticated as Azure
Document Intelligence, but they can be used to decrease charges.</p>
<table>
<colgroup>
<col style="width: 9%" />
<col style="width: 54%" />
<col style="width: 36%" />
</colgroup>
<thead>
<tr>
<th>Format</th>
<th>Manual indexing</th>
<th>Integrated Vectorization</th>
</tr>
</thead>
<tbody>
<tr>
<td>PDF</td>
<td>Yes (DI or local with PyPDF)</td>
<td>Yes</td>
</tr>
<tr>
<td>HTML</td>
<td>Yes (DI or local with BeautifulSoup)</td>
<td>Yes</td>
</tr>
<tr>
<td>DOCX, PPTX, XLSX</td>
<td>Yes (DI)</td>
<td>Yes</td>
</tr>
<tr>
<td>Images (JPG, PNG, BPM, TIFF, HEIFF)</td>
<td>Yes (DI)</td>
<td>Yes</td>
</tr>
<tr>
<td>TXT</td>
<td>Yes (Local)</td>
<td>Yes</td>
</tr>
<tr>
<td>JSON</td>
<td>Yes (Local)</td>
<td>Yes</td>
</tr>
<tr>
<td>CSV</td>
<td>Yes (Local)</td>
<td>Yes</td>
</tr>
</tbody>
</table>
<p>The Blob indexer used by the Integrated Vectorization approach also
supports a few <a
href="https://learn.microsoft.com/azure/search/search-howto-indexing-azure-blob-storage#supported-document-formats">additional
formats</a>.</p>
<h2 id="manual-indexing-process">Manual indexing process</h2>
<p>The <a href="../app/backend/prepdocs.py"><code>prepdocs.py</code></a>
script is responsible for both uploading and indexing documents. The
typical usage is to call it using <code>scripts/prepdocs.sh</code>
(Mac/Linux) or <code>scripts/prepdocs.ps1</code> (Windows), as these
scripts will set up a Python virtual environment and pass in the
required parameters based on the current <code>azd</code> environment.
You can pass additional arguments directly to the script, for example
<code>scripts/prepdocs.ps1 --removeall</code>. Whenever
<code>azd up</code> or <code>azd provision</code> is run, the script is
called automatically.</p>
<figure>
<img src="images/diagram_prepdocs.png"
alt="Diagram of the indexing process" />
<figcaption aria-hidden="true">Diagram of the indexing
process</figcaption>
</figure>
<p>The script uses the following steps to index documents:</p>
<ol type="1">
<li>If it doesn’t yet exist, create a new index in Azure AI Search.</li>
<li>Upload the PDFs to Azure Blob Storage.</li>
<li>Split the PDFs into chunks of text.</li>
<li>Upload the chunks to Azure AI Search. If using vectors (the
default), also compute the embeddings and upload those alongside the
text.</li>
</ol>
<h3 id="chunking">Chunking</h3>
<p>We’re often asked why we need to break up the PDFs into chunks when
Azure AI Search supports searching large documents.</p>
<p>Chunking allows us to limit the amount of information we send to
OpenAI due to token limits. By breaking up the content, it allows us to
easily find potential chunks of text that we can inject into OpenAI. The
method of chunking we use leverages a sliding window of text such that
sentences that end one chunk will start the next. This allows us to
reduce the chance of losing the context of the text.</p>
<p>If needed, you can modify the chunking algorithm in
<code>app/backend/prepdocslib/textsplitter.py</code>.</p>
<h3
id="enhancing-search-functionality-with-data-categorization">Enhancing
search functionality with data categorization</h3>
<p>To enhance search functionality, categorize data during the ingestion
process with the <code>--category</code> argument, for example
<code>scripts/prepdocs.ps1 --category ExampleCategoryName</code>. This
argument specifies the category to which the data belongs, enabling you
to filter search results based on these categories.</p>
<p>After running the script with the desired category, ensure these
categories are added to the ‘Include Category’ dropdown list. This can
be found in the developer settings in <a
href="https://github.com/Azure-Samples/azure-search-openai-demo/blob/main/app/frontend/src/components/Settings/Settings.tsx"><code>Settings.tsx</code></a>.
The default option for this dropdown is “All”. By including specific
categories, you can refine your search results more effectively.</p>
<h3 id="indexing-additional-documents">Indexing additional
documents</h3>
<p>To upload more PDFs, put them in the data/ folder and run
<code>./scripts/prepdocs.sh</code> or
<code>./scripts/prepdocs.ps1</code>.</p>
<p>A <a
href="https://github.com/Azure-Samples/azure-search-openai-demo/pull/835">recent
change</a> added checks to see what’s been uploaded before. The prepdocs
script now writes an .md5 file with an MD5 hash of each file that gets
uploaded. Whenever the prepdocs script is re-run, that hash is checked
against the current hash and the file is skipped if it hasn’t
changed.</p>
<h3 id="removing-documents">Removing documents</h3>
<p>You may want to remove documents from the index. For example, if
you’re using the sample data, you may want to remove the documents that
are already in the index before adding your own.</p>
<p>To remove all documents, use
<code>./scripts/prepdocs.sh --removeall</code> or
<code>./scripts/prepdocs.ps1 --removeall</code>.</p>
<p>You can also remove individual documents by using the
<code>--remove</code> flag. Open either <code>scripts/prepdocs.sh</code>
or <code>scripts/prepdocs.ps1</code> and replace <code>/data/*</code>
with <code>/data/YOUR-DOCUMENT-FILENAME-GOES-HERE.pdf</code>. Then run
<code>scripts/prepdocs.sh --remove</code> or
<code>scripts/prepdocs.ps1 --remove</code>.</p>
<h2 id="integrated-vectorization">Integrated Vectorization</h2>
<p>Azure AI Search includes an <a
href="https://techcommunity.microsoft.com/blog/azure-ai-services-blog/announcing-the-public-preview-of-integrated-vectorization-in-azure-ai-search/3960809">integrated
vectorization feature</a>, a cloud-based approach to data ingestion.
Integrated vectorization takes care of document format cracking, data
extraction, chunking, vectorization, and indexing, all with Azure
technologies.</p>
<p>See <a
href="https://github.com/Azure/azure-search-vector-samples/blob/main/demo-python/code/integrated-vectorization/azure-search-integrated-vectorization-sample.ipynb">this
notebook</a> to understand the process of setting up integrated
vectorization. We have integrated that code into our
<code>prepdocs</code> script, so you can use it without needing to
understand the details.</p>
<p>You must first explicitly <a
href="./deploy_features.md#enabling-integrated-vectorization">enable
integrated vectorization</a> in the <code>azd</code> environment to use
this feature.</p>
<p>This feature cannot be used on existing index. You need to create a
new index or drop and recreate an existing index. In the newly created
index schema, a new field ‘parent_id’ is added. This is used internally
by the indexer to manage life cycle of chunks.</p>
<p>This feature is not supported in the free SKU for Azure AI
Search.</p>
<h3 id="indexing-of-additional-documents">Indexing of additional
documents</h3>
<p>To add additional documents to the index, first upload them to your
data source (Blob storage, by default). Then navigate to the Azure
portal, find the index, and run it. The Azure AI Search indexer will
identify the new documents and ingest them into the index.</p>
<h3 id="removal-of-documents">Removal of documents</h3>
<p>To remove documents from the index, remove them from your data source
(Blob storage, by default). Then navigate to the Azure portal, find the
index, and run it. The Azure AI Search indexer will take care of
removing those documents from the index.</p>
<h3 id="scheduled-indexing">Scheduled indexing</h3>
<p>If you would like the indexer to run automatically, you can set it up
to <a
href="https://learn.microsoft.com/azure/search/search-howto-schedule-indexers">run
on a schedule</a>.</p>
<h2 id="debugging-tips">Debugging tips</h2>
<p>If you are not sure if a file successfully uploaded, you can query
the index from the Azure Portal or from the REST API. Open the index and
paste the queries below into the search bar.</p>
<p>To see all the filenames uploaded to the index:</p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode json"><code class="sourceCode json"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>  <span class="dt">&quot;search&quot;</span><span class="fu">:</span> <span class="st">&quot;*&quot;</span><span class="fu">,</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>  <span class="dt">&quot;count&quot;</span><span class="fu">:</span> <span class="kw">true</span><span class="fu">,</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>  <span class="dt">&quot;top&quot;</span><span class="fu">:</span> <span class="dv">1</span><span class="fu">,</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>  <span class="dt">&quot;facets&quot;</span><span class="fu">:</span> <span class="ot">[</span><span class="st">&quot;sourcefile&quot;</span><span class="ot">]</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="fu">}</span></span></code></pre></div>
<p>To search for specific filenames:</p>
<div class="sourceCode" id="cb2"><pre
class="sourceCode json"><code class="sourceCode json"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>  <span class="dt">&quot;search&quot;</span><span class="fu">:</span> <span class="st">&quot;*&quot;</span><span class="fu">,</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>  <span class="dt">&quot;count&quot;</span><span class="fu">:</span> <span class="kw">true</span><span class="fu">,</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>  <span class="dt">&quot;top&quot;</span><span class="fu">:</span> <span class="dv">1</span><span class="fu">,</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>  <span class="dt">&quot;filter&quot;</span><span class="fu">:</span> <span class="st">&quot;sourcefile eq &#39;employee_handbook.pdf&#39;&quot;</span><span class="fu">,</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>  <span class="dt">&quot;facets&quot;</span><span class="fu">:</span> <span class="ot">[</span><span class="st">&quot;sourcefile&quot;</span><span class="ot">]</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="fu">}</span></span></code></pre></div>
